{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Data\n"
      ],
      "metadata": {
        "id": "9Ex0_KYmYquK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM_BsjlJTnzx"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('news.xlsx')\n",
        "df"
      ],
      "metadata": {
        "id": "N2UIs5qaU0ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "_oUrdqAEY3II"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hapus Missing Value"
      ],
      "metadata": {
        "id": "R9mPVWD3Y5Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fancyimpute"
      ],
      "metadata": {
        "id": "tyxwVN1MVXGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from fancyimpute import IterativeImputer\n",
        "\n",
        "positivity = df['positivity']\n",
        "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
        "positivity_imputed = imputer.fit_transform(positivity.values.reshape(-1, 1))\n"
      ],
      "metadata": {
        "id": "xUFmjWDHZznU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['positivity'] = positivity_imputed\n",
        "df"
      ],
      "metadata": {
        "id": "sGj5pO8kaqFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung jumlah missing value per kolom\n",
        "missing_values = df.isna().sum()\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "JOofaiwgcKRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['positivity_gold'])\n",
        "df = df.drop(columns=['relevance_gold'])\n",
        "df = df.drop(columns=['Unnamed: 15'])\n",
        "df = df.drop(columns=['Unnamed: 16'])\n",
        "df = df.drop(columns=['Unnamed: 17'])\n",
        "df = df.drop(columns=['Unnamed: 18'])\n",
        "df = df.drop(columns=['Unnamed: 19'])\n",
        "df = df.drop(columns=['Unnamed: 20'])\n",
        "df = df.drop(columns=['Unnamed: 21'])\n",
        "df = df.drop(columns=['Unnamed: 22'])\n",
        "df = df.drop(columns=['Unnamed: 23'])"
      ],
      "metadata": {
        "id": "PRylHomD65qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode_value = df['text'].mode()[0] #pake modus\n",
        "df['text'].fillna(mode_value, inplace=True)\n"
      ],
      "metadata": {
        "id": "d8hkL6YFdMij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "eMod8yaefi1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "!pip install PySastrawi\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ],
      "metadata": {
        "id": "Loq2XlRs70zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "text = df[\"text\"].to_string()\n",
        "text"
      ],
      "metadata": {
        "id": "r4xaKGPV707V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Menggunakan Stopwords dan Stemming"
      ],
      "metadata": {
        "id": "Ivmhlwk--Bcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus Stopword\n",
        "factory = StopWordRemoverFactory()\n",
        "stopword = factory.create_stop_word_remover()\n",
        "\n",
        "stop = stopword.remove(text.lower())\n",
        "\n",
        "# Stemming\n",
        "stemmer = StemmerFactory().create_stemmer()\n",
        "stem = stemmer.stem(stop)"
      ],
      "metadata": {
        "id": "6_5Fwq2j8TRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hapus Karakter Tertentu"
      ],
      "metadata": {
        "id": "5qvhS7xL-Hy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rm_punct2(text):\n",
        "    return re.sub(\n",
        "        r'[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]',\n",
        "        ' ', text)\n",
        "\n",
        "def rm_html(text):\n",
        "    return re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "def rm_number(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "def rm_whitespaces(text):\n",
        "    return re.sub(r' +', ' ', text)\n",
        "\n",
        "def rm_nonascii(text):\n",
        "    return re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
        "\n",
        "def rm_emoji(text):\n",
        "    emojis = re.compile(\n",
        "        '['\n",
        "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
        "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
        "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
        "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
        "        u'\\U00002702-\\U000027B0'\n",
        "        u'\\U000024C2-\\U0001F251'\n",
        "        ']+',\n",
        "        flags=re.UNICODE\n",
        "    )\n",
        "    return emojis.sub(r'', text)\n",
        "\n",
        "def spell_correction(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
        "\n",
        "def clean_pipeline(text):\n",
        "    no_html = rm_html(text)\n",
        "    no_punct = rm_punct2(no_html)\n",
        "    no_number = rm_number(no_punct)\n",
        "    no_whitespaces = rm_whitespaces(no_number)\n",
        "    no_nonasci = rm_nonascii(no_whitespaces)\n",
        "    no_emoji = rm_emoji(no_nonasci)\n",
        "    spell_corrected = spell_correction(no_emoji)\n",
        "    clean_lowered = spell_corrected.lower()\n",
        "    return clean_lowered\n",
        "\n",
        "cleaned = clean_pipeline(stem)\n",
        "cleaned"
      ],
      "metadata": {
        "id": "ywf95Ykp9opP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisasi Menggunakan TextBlob"
      ],
      "metadata": {
        "id": "mc9dWpAC-P3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenisasi kata dengan TextBlob\n",
        "tokens = TextBlob(cleaned).words\n",
        "print(tokens)\n",
        "\n",
        "fdist = FreqDist(tokens)\n",
        "print(fdist)\n",
        "\n",
        "fdist1 = fdist.most_common(10)\n",
        "print(fdist1)"
      ],
      "metadata": {
        "id": "VdISudHc9y3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mengganti Slang & Kata Singkatan"
      ],
      "metadata": {
        "id": "srQng6IBE1Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abbreviation_dict = {\n",
        "    \"abt\": \"about\",\n",
        "    \"afk\": \"away from keyboard\",\n",
        "    \"asap\": \"as soon as possible\",\n",
        "    \"bday\": \"birthday\",\n",
        "    \"brb\": \"be right back\",\n",
        "    \"btw\": \"by the way\",\n",
        "    \"cya\": \"see you\",\n",
        "    \"diy\": \"do it yourself\",\n",
        "    \"fwiw\": \"for what it's worth\",\n",
        "    \"fyi\": \"for your information\",\n",
        "    \"gr8\": \"great\",\n",
        "    \"gtg\": \"got to go\",\n",
        "    \"idk\": \"I don't know\",\n",
        "    \"irl\": \"in real life\",\n",
        "    \"lmao\": \"laughing my ass off\",\n",
        "    \"lol\": \"laugh out loud\",\n",
        "    \"np\": \"no problem\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"plz\": \"please\",\n",
        "    \"rofl\": \"rolling on the floor laughing\",\n",
        "    \"smh\": \"shaking my head\",\n",
        "    \"thx\": \"thanks\",\n",
        "    \"tmi\": \"too much information\",\n",
        "    \"ttyl\": \"talk to you later\",\n",
        "    \"wtf\": \"what the f***\",\n",
        "    \"yolo\": \"you only live once\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "epbZMKTsE7kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expanded_text = []\n",
        "for token in tokens:\n",
        "    expanded_text.append(abbreviation_dict.get(token.lower(), token))\n",
        "\n",
        "cleaned_text = \" \".join(expanded_text)"
      ],
      "metadata": {
        "id": "d7ZldJh3E_8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisasi Hasil Preprocessing Menggunakan Wordcloud"
      ],
      "metadata": {
        "id": "7sN8SfBKFc5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Menghitung bobot TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([' '.join(expanded_text)])\n",
        "\n",
        "# Membuat Word Cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
        "wordcloud.generate_from_frequencies({kata: bobot for kata, bobot in\n",
        "                                     zip(tfidf_vectorizer.get_feature_names_out(),\n",
        "                                         tfidf_matrix.toarray()[0])})\n",
        "\n",
        "# Menampilkan Word Cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TgiOnoMsFjJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Term Weighting menggunakan Word Embedding (Word2Vec)"
      ],
      "metadata": {
        "id": "i55Zko0AKbVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "LosMvRe6IB92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing all necessary modules\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "# Replaces escape character with space\n",
        "f = cleaned_text.replace(\"\\n\", \" \")\n",
        "\n",
        "data = []\n",
        "\n",
        "# iterate through each sentence in the file\n",
        "for i in sent_tokenize(f):\n",
        "    temp = []\n",
        "\n",
        "    # tokenize the sentence into words\n",
        "    for j in word_tokenize(i):\n",
        "        temp.append(j.lower())\n",
        "\n",
        "    data.append(temp)\n",
        "\n",
        "# Create CBOW model\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1,\n",
        "                              vector_size = 100, window = 5)\n",
        "\n",
        "# Print results\n",
        "print(\"Cosine similarity between 'the' \" +\n",
        "               \"and 'administration' - CBOW : \",\n",
        "    model1.wv.similarity('the', 'administration'))\n",
        "\n",
        "# Create Skip Gram model\n",
        "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100,\n",
        "                                             window = 5, sg = 1)\n",
        "\n",
        "# Print results\n",
        "print(\"Cosine similarity between 'the' \" +\n",
        "          \"and 'administration' - Skip Gram : \",\n",
        "    model2.wv.similarity('the', 'administration'))\n"
      ],
      "metadata": {
        "id": "A6FxUhzBJtvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "oMGNP1H5Tsko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "text = list(df['text'])\n",
        "\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "corpus = []\n",
        "\n",
        "for i in range(len(text)):\n",
        "    text[i] = str(text[i])\n",
        "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
        "\n",
        "    r = r.lower()\n",
        "\n",
        "    r = r.split()\n",
        "\n",
        "    r = [word for word in r if word not in stopwords.words('english')]\n",
        "\n",
        "    r = [lemmatizer.lemmatize(word) for word in r]\n",
        "\n",
        "    r = ' '.join(r)\n",
        "\n",
        "    corpus.append(r)\n",
        "\n",
        "#assign corpus to data['text']\n",
        "\n",
        "df['text'] = corpus\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "ZKxAvwB4Kozh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "COWTFNsnsK2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Test Split"
      ],
      "metadata": {
        "id": "gOk80-fep_RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Feature and Label sets\n",
        "\n",
        "X = df['text']\n",
        "y = df['relevance']\n",
        "\n",
        "# train test split (66% train - 33% test)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
        "print('Training Data :', X_train.shape)\n",
        "print('Testing Data : ', X_test.shape)"
      ],
      "metadata": {
        "id": "MDjF41RPkF6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction"
      ],
      "metadata": {
        "id": "gi2ojcoKp6tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Bag of Words model\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()\n",
        "\n",
        "X_train_cv = cv.fit_transform(X_train)\n",
        "\n",
        "X_train_cv.shape"
      ],
      "metadata": {
        "id": "HF9zZJvPqB-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training & Evaluation"
      ],
      "metadata": {
        "id": "-_yF_VGdrerz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Logistic Regression model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "lr.fit(X_train_cv, y_train)\n",
        "\n",
        "\n",
        "# transform X_test using CV\n",
        "\n",
        "X_test_cv = cv.transform(X_test)\n",
        "\n",
        "\n",
        "# generate predictions\n",
        "\n",
        "predictions = lr.predict(X_test_cv)\n",
        "\n",
        "predictions"
      ],
      "metadata": {
        "id": "SEGmIL0yriPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "ZoBT_8JrsyWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "df = pd.DataFrame(metrics.confusion_matrix(y_test,predictions), index=['yes','no', 'no'], columns=['no','no','no'])\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "mhTBk40xrqzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest Classifier"
      ],
      "metadata": {
        "id": "3kQS8N1ZtozK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train_cv, y_train)\n",
        "rf_predictions = rf.predict(X_test_cv)\n"
      ],
      "metadata": {
        "id": "7WJozlrKtOGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy Score"
      ],
      "metadata": {
        "id": "0LIm9G2rtsf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Menghitung akurasi\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "print(\"Random Forest Classifier Accuracy:\", rf_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "8Tf3kYKztfMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "vCYR7o3Etyow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "rf_confusion_matrix = confusion_matrix(y_test, rf_predictions)\n",
        "print(\"Confusion Matrix for Random Forest Classifier:\")\n",
        "print(rf_confusion_matrix)"
      ],
      "metadata": {
        "id": "gXL25Pnmth8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "ElixG_mVt-BW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_cv, y_train)\n",
        "nb_predictions = nb.predict(X_test_cv)\n"
      ],
      "metadata": {
        "id": "0kqhPrwit73V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy Score"
      ],
      "metadata": {
        "id": "wSxZ6xCsuQVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Menghitung akurasi model\n",
        "accuracy = accuracy_score(y_test, nb_predictions)\n",
        "print(\"Akurasi Model Naive Bayes:\", accuracy)\n"
      ],
      "metadata": {
        "id": "ZSVpqO3kuS6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat confusion matrix\n",
        "confusion = confusion_matrix(y_test, nb_predictions)\n",
        "print(\"Confusion Matrix Model Naive Bayes:\")\n",
        "print(confusion)"
      ],
      "metadata": {
        "id": "Ik6W5zxpuUuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machine (SVM) Classifier"
      ],
      "metadata": {
        "id": "EyJBUS51yleH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC()\n",
        "svm.fit(X_train_cv, y_train)\n",
        "svm_predictions = svm.predict(X_test_cv)\n"
      ],
      "metadata": {
        "id": "qq6BlOfKyoFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy Score"
      ],
      "metadata": {
        "id": "zpgEGQsJy0Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Hitung akurasi\n",
        "accuracy = accuracy_score(y_test, svm_predictions)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "id": "ikJ2jDZryz1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "MeEb4_xcy3Bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hitung confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, svm_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "IFyiLyhxy7iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Nearest Neighbors (KNN) Classifier"
      ],
      "metadata": {
        "id": "c5WDTLoky_Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train_cv, y_train)\n",
        "knn_predictions = knn.predict(X_test_cv)\n"
      ],
      "metadata": {
        "id": "37rqs1pmy9yG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy Score"
      ],
      "metadata": {
        "id": "_t1JJZdAzGsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Menghitung akurasi\n",
        "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
        "print(\"K-Nearest Neighbors (KNN) Accuracy:\", knn_accuracy)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZBVDZHjNzJd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "D0g3PJRzzJuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat confusion matrix\n",
        "knn_confusion_matrix = confusion_matrix(y_test, knn_predictions)\n",
        "print(\"K-Nearest Neighbors (KNN) Confusion Matrix:\")\n",
        "print(knn_confusion_matrix)"
      ],
      "metadata": {
        "id": "mT8xx67kzQM9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}